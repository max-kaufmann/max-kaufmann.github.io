[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Kaufmann",
    "section": "",
    "text": "I’m an ML researcher, interested in ensuring that AI progress is broadly beneficial—both by minimising AI-induced harms, and ensuring that the benefits are shared outside of the privileged few.\nI’m currently working within the UK government, as part of the founding team at UK’s AI Safety Institute - where I’m building out our agent infrastructure. I’m also an incoming PhD student at the University of Toronto, supervised by Roger Grosse.\nPreviously I worked at the Center for AI Safety, was a research intern at UC Berkeley, studied LLM generalization under Owain Evans and completed my CS undergrad at the University of Cambridge."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Max Kaufmann",
    "section": "",
    "text": "I’m an ML researcher, interested in ensuring that AI progress is broadly beneficial—both by minimising AI-induced harms, and ensuring that the benefits are shared outside of the privileged few.\nI’m currently working within the UK government, as part of the founding team at UK’s AI Safety Institute - where I’m building out our agent infrastructure. I’m also an incoming PhD student at the University of Toronto, supervised by Roger Grosse.\nPreviously I worked at the Center for AI Safety, was a research intern at UC Berkeley, studied LLM generalization under Owain Evans and completed my CS undergrad at the University of Cambridge."
  },
  {
    "objectID": "index.html#recent-works",
    "href": "index.html#recent-works",
    "title": "Max Kaufmann",
    "section": "Recent Works",
    "text": "Recent Works"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Max Kaufmann",
    "section": "",
    "text": "My current research focuses on developing techinical interventions which enable the governance of increasingly agentic AI systems, with a particular interest in the pre-deployment auditing and the post-deployment monitoring of LLM agents.\nPreviously I’ve worked on:\n\nAdversarial Robustness: Can we develop new threat models for adversarial robustness, that better capture what we care about? How can we make adversarial training more efficient?\nLLM Generalization: What are the limits of LLM generalization? How does strong generalisation ability link to concerns around the situational awareness of these systems?\nLLM Evals: Do AI systems pose biosecurity risks? How cam we effectively measure the capabilities of LLM agents?\n\nRight now, I’m thinking about:\n\nImproving LLM agents: What is the equivalent of DPO (i.e. an effective, simple, scalable finetuning technique) for LLM agents? What should we aim for when designing an evaluation suite that allows for fast iteration on the capabilities of LLM agents?\nMonitoring agentic AI systems: Which technical frameworks are needed to effectively monitor wide deployments of AI agents? Can we design monitoring frameworks which don’t centralize power?\nEnsuring global access to AI: How can we accelerate the global south’s participation in AI development? What role should western goverments play in widely spreading the benefits of AI?\n\nPlease reach out if you think we could collaborate!"
  },
  {
    "objectID": "research/index.html#research",
    "href": "research/index.html#research",
    "title": "Max Kaufmann",
    "section": "",
    "text": "My current research focuses on developing techinical interventions which enable the governance of increasingly agentic AI systems, with a particular interest in the pre-deployment auditing and the post-deployment monitoring of LLM agents.\nPreviously I’ve worked on:\n\nAdversarial Robustness: Can we develop new threat models for adversarial robustness, that better capture what we care about? How can we make adversarial training more efficient?\nLLM Generalization: What are the limits of LLM generalization? How does strong generalisation ability link to concerns around the situational awareness of these systems?\nLLM Evals: Do AI systems pose biosecurity risks? How cam we effectively measure the capabilities of LLM agents?\n\nRight now, I’m thinking about:\n\nImproving LLM agents: What is the equivalent of DPO (i.e. an effective, simple, scalable finetuning technique) for LLM agents? What should we aim for when designing an evaluation suite that allows for fast iteration on the capabilities of LLM agents?\nMonitoring agentic AI systems: Which technical frameworks are needed to effectively monitor wide deployments of AI agents? Can we design monitoring frameworks which don’t centralize power?\nEnsuring global access to AI: How can we accelerate the global south’s participation in AI development? What role should western goverments play in widely spreading the benefits of AI?\n\nPlease reach out if you think we could collaborate!"
  },
  {
    "objectID": "research/index.html#works",
    "href": "research/index.html#works",
    "title": "Max Kaufmann",
    "section": "Works",
    "text": "Works\n\n\n\nThe Reversal Curse: LLMs trained on “A is B” fail to learn “B is A” Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans [arxiv] [tweet]\nTaken out of context: On measuring situational awareness in LLMs Lukas Berglund*, Asa Cooper Stickland*, Mikita Balesni*, Max Kaufmann*, Meg Tong*, Thomas Korbak, D. Kokotajlo, O. Evans [arxiv] [tweet]\nTesting Robustness Against Unforeseen Adversaries Max Kaufmann*, Daniel Kang*, Yi Sun*, Steven Basart, Xuwang Yin, Mantas Mazeika, Akul Arora, Adam Dziedzic, Franziska Boenisch, Tom Brown, Jacob Steinhardt, Dan Hendrycks [arxiv]\nEfficient Adversarial Training With Data Pruning Max Kaufmann, Yiren Zhao, Ilia Shumailov, Robert Mullins, Nicolas Papernot [arxiv]\nDual-use biology capabilities across model scale Max Kaufmann, Gryphon Scientific, Jonas Sandbrink Presented to policymakers at the 2023 International AI Safety Summit. \nMatAttack: Differential materials for adversarial attacks Dron Hazra*, Max Kaufmann*, Dan Hendrycks forthcoming"
  }
]